# homework1
使用Python语言，用Jupyter Notebook作为编辑器，基于Pytorch框架以及CIFAR 10训练集下构建的一个基础神经网络模型，主要是通过调整超参数来观察各个超参数的作用，以及超参数对训练所产生的不同影响

流程：
1.通过torchvision.datasets包加载数据集  
2.建立一个具有2层卷积、2层池化、3层全连接的CNN模型
3.设置超参  
4.训练模型并不断调整超参
5.通过测试集数据检验模型在全部类和各类上的准确率值。  

超参基本设置
* Learing Rate = 0.01
* Epoch = 5
* Batch Size = 4
* Optimizer：SGD(momentum=0.9)
* Criterion：CrossEntropyLoss

#调节超参
##调节epoch  
 
| epoch    |  acc  |
| --------| :----: |
| 5      |   15%    |
| 50     |   24%    |
| 100    |   49%    |
| 200    |    51%   |
epoch acc 
5   15%  
50	24%  
100  49%
200 51%

可以看出随着EPOCH的增加，我们的模型效果是在不断变好的，但是我们的EPOCH也不能设置的太大，因为训练过多的话，反而会产生过拟合，使得我们的泛化能力变差，像后面增加的EPOCH所带来的的ACC提升就没有一开始的明显

在上述调整EPOCH大小的时候，我们观察loss曲线可以看出虽然总体loss是在下降的，但是在下降的过程中上下波动不小，所以我们对普通的SGD进行替换，使用在SGD基础上增加了一阶动量和二阶动量的Adam优化器。
 
##调节学习率LR

LR=0.001

| 水果        | 价格    |  数量  |
| --------   | -----:   | :----: |
| 香蕉        | $1      |   5    |
| 苹果        | $1      |   6    |
| 草莓        | $1      |   7    |

epoch acc
50 43%
100 58%

学习率： 习率是指导我们，在梯度下降法中，如何使用损失函数的梯度调整网络权重的超参数。学习率如果过大，可能会使损失函数直接越过全局最优点，学习率如果过小，损失函数的变化速度很慢，会大大增加网络的收敛复杂度，并且很容易被困在局部最小值或者鞍点

LR = 0.001

EPOCH	LOSS	ACC
50	1.11	58.58%
100	0.87	63.05%
可以上面的结果进行比较，可以看到训练100个EPOCH时的数据还是有明显提升的，可见学习率调小后，说明模型收敛的速度在减慢，减缓了发生过拟合的速度

总结
各个不同的超参数之间会对模型的训练产生不同的影响 ，不能一味的叠加。需要我们根据模型的训练过程以及数据来进行一个动态调整






